{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import re\n",
    "import csv\n",
    "import string\n",
    "from sentistrength import PySentiStr\n",
    "from scipy.sparse import hstack\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import accuracy_score\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from skmultilearn.problem_transform import ClassifierChain\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "from skmultilearn.problem_transform import LabelPowerset\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPLACE_BY_SPACE_RE = re.compile('[/(){}\\[\\]\\|@,;]')\n",
    "BAD_SYMBOLS_RE = re.compile('[^0-9a-z #+_]')\n",
    "stopword = stopwords.words('english')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "senti = PySentiStr()\n",
    "senti.setSentiStrengthPath('SentiStrength.jar')\n",
    "senti.setSentiStrengthLanguageFolderPath('SentiStrength_Data/')\n",
    "\n",
    "f1 = open(\"stop_word_list.txt\", \"r\")\n",
    "stop_data = f1.read()\n",
    "stop_list = stop_data.split('\\n')\n",
    "f1.close()\n",
    "\n",
    "def clean_text(text):\n",
    "\ttext = ''.join(c for c in text if not c.isdigit())\n",
    "\ttext = ''.join(c for c in text if c not in string.punctuation)\n",
    "\ttext = text.lower() # lowercase text\n",
    "\ttext = REPLACE_BY_SPACE_RE.sub(' ', text) # replace REPLACE_BY_SPACE_RE symbols by space in text\n",
    "\ttext = BAD_SYMBOLS_RE.sub('', text) # delete symbols which are in BAD_SYMBOLS_RE from text\n",
    "\treturn text\n",
    "\n",
    "def get_verbs(text):\n",
    "\tpresent = 0\n",
    "\tpast = 0\n",
    "\tfuture = 0\n",
    "\tverb = 0\n",
    "\twordsList = nltk.word_tokenize(text)\n",
    "\twordsList = [w for w in wordsList if not w in stopword]\n",
    "\ttagged = nltk.pos_tag(wordsList)\n",
    "\tfor pos in tagged:\n",
    "\t\tif pos[1] in ['VBG', 'VBP', 'VBZ']:\n",
    "\t\t\tpresent += 1\n",
    "\t\t\tverb += 1\n",
    "\t\telif pos[1] in ['VBD', 'VBN']:\n",
    "\t\t\tpast += 1\n",
    "\t\t\tverb += 1\n",
    "\t\telif pos[1] in ['VB']:\n",
    "\t\t\tfuture += 1\n",
    "\t\t\tverb += 1\n",
    "\tif verb == 0:\n",
    "\t\treturn 0, 0, 0\n",
    "\tpresent /= verb\n",
    "\tpast /= verb\n",
    "\tfuture /= verb\n",
    "\treturn present, past, future\n",
    "\n",
    "def get_present(rev_str):\n",
    "\treturn get_verbs(rev_str)[0]\n",
    "\n",
    "def get_past(rev_str):\n",
    "\treturn get_verbs(rev_str)[1]\n",
    "\n",
    "def get_future(rev_str):\n",
    "\treturn get_verbs(rev_str)[2]\n",
    "\n",
    "def get_senti_score(text):\n",
    "\tresult = senti.getSentiment(text, score='dual')\n",
    "\treturn result\n",
    "\n",
    "def get_pos_senti_score(rev_str):\n",
    "\treturn get_senti_score(rev_str)[0][0]\n",
    "\n",
    "def get_neg_senti_score(rev_str):\n",
    "\treturn get_senti_score(rev_str)[0][1]\n",
    "\n",
    "def length_text(text):\n",
    "\tlength = len(text)\n",
    "\treturn length\n",
    "\n",
    "def process_text(text):\n",
    "\twords = nltk.word_tokenize(text)\n",
    "\tremoved_stopwords = [word for word in words if word not in stopword]\n",
    "\tremoved_stopwords = [word for word in removed_stopwords if word not in stop_list]\n",
    "\tlemmatized = [lemmatizer.lemmatize(word) for word in removed_stopwords]\n",
    "\trefined_review = ' '.join([str(elem) for elem in lemmatized])\n",
    "\treturn refined_review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"labelled.csv\")\n",
    "col = ['Review', 'UpVotes', 'Rating', 'Label']\n",
    "dataset = dataset[col]\n",
    "dataset = dataset[pd.notnull(dataset['Review'])]\n",
    "\n",
    "dataset['Review'] = dataset['Review'].apply(clean_text)\n",
    "dataset['Length'] = dataset['Review'].apply(length_text)\n",
    "dataset['Positive'] = dataset['Review'].apply(get_pos_senti_score)\n",
    "dataset['Negative'] = dataset['Review'].apply(get_neg_senti_score)\n",
    "dataset['Present'] = dataset['Review'].apply(get_present)\n",
    "dataset['Past'] = dataset['Review'].apply(get_past)\n",
    "dataset['Future'] = dataset['Review'].apply(get_future)\n",
    "dataset['Review'] = dataset['Review'].apply(process_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "multilabel_binarizer = MultiLabelBinarizer()\n",
    "multilabel_binarizer.fit_transform(dataset['Label'])\n",
    "\n",
    "y = multilabel_binarizer.transform(dataset['Label'])\n",
    "\n",
    "for idx, label in enumerate(multilabel_binarizer.classes_):\n",
    "    dataset[label] = y[:,idx]\n",
    "\n",
    "dataset.to_csv('dataset_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
